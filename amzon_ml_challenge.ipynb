{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport csv\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nimport torch\n\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nfrom torch.cuda import amp\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:00:33.240696Z","iopub.execute_input":"2021-08-01T17:00:33.241135Z","iopub.status.idle":"2021-08-01T17:00:40.970479Z","shell.execute_reply.started":"2021-08-01T17:00:33.241044Z","shell.execute_reply":"2021-08-01T17:00:40.969472Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"'''import gdown\n\nurl = 'https://drive.google.com/uc?id=1B6_rtcmGRy49hqpwoJT-_Ujnt6cYj5Ba'\n\noutput = 'file.npy'\n\ngdown.download(url, output, quiet=False)'''","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:20:10.062437Z","iopub.execute_input":"2021-08-01T15:20:10.062767Z","iopub.status.idle":"2021-08-01T15:20:10.077107Z","shell.execute_reply.started":"2021-08-01T15:20:10.062732Z","shell.execute_reply":"2021-08-01T15:20:10.075564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/amazon-ml-challenge-2021-hackerearth/train.csv\", error_bad_lines = False, escapechar = \"\\\\\" , quoting = csv.QUOTE_NONE)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:20:10.079543Z","iopub.execute_input":"2021-08-01T15:20:10.080207Z","iopub.status.idle":"2021-08-01T15:21:29.280436Z","shell.execute_reply.started":"2021-08-01T15:20:10.080161Z","shell.execute_reply":"2021-08-01T15:21:29.279451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna(subset=['TITLE'])\ndf = df.reset_index(drop = True)\ndf = df.drop_duplicates()\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:21:29.28211Z","iopub.execute_input":"2021-08-01T15:21:29.282545Z","iopub.status.idle":"2021-08-01T15:21:39.712207Z","shell.execute_reply.started":"2021-08-01T15:21:29.282503Z","shell.execute_reply":"2021-08-01T15:21:39.711327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_values=dict()\ntrain  = list(df['BROWSE_NODE_ID'])\ncounter= 0\nfor i in train:\n  if map_values.get(i)==None:\n    map_values[i]=counter\n    counter+=1\ndf['BROWSE_NODE_ID'] = [map_values[x] for x in train]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:21:39.713466Z","iopub.execute_input":"2021-08-01T15:21:39.71384Z","iopub.status.idle":"2021-08-01T15:21:41.981575Z","shell.execute_reply.started":"2021-08-01T15:21:39.713798Z","shell.execute_reply":"2021-08-01T15:21:41.980676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=50)\nsplit_df = skf.split(df,df['BROWSE_NODE_ID'])\ndel split_df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:21:41.982968Z","iopub.execute_input":"2021-08-01T15:21:41.983402Z","iopub.status.idle":"2021-08-01T15:21:41.988942Z","shell.execute_reply.started":"2021-08-01T15:21:41.983343Z","shell.execute_reply":"2021-08-01T15:21:41.987912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df[\"BROWSE_NODE_ID\"].unique())","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:21:41.990403Z","iopub.execute_input":"2021-08-01T15:21:41.990866Z","iopub.status.idle":"2021-08-01T15:21:42.024264Z","shell.execute_reply.started":"2021-08-01T15:21:41.990831Z","shell.execute_reply":"2021-08-01T15:21:42.023382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(df['BROWSE_NODE_ID']))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:21:42.051474Z","iopub.execute_input":"2021-08-01T15:21:42.051801Z","iopub.status.idle":"2021-08-01T15:21:42.473027Z","shell.execute_reply.started":"2021-08-01T15:21:42.05177Z","shell.execute_reply":"2021-08-01T15:21:42.472104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train,df_test,x,y = train_test_split(df,df['BROWSE_NODE_ID'],test_size=0.05)\ndel df_train\ndel x\ndel df\n\ndf=df_test\ndf = df.reset_index(drop = True)\nlen(df[\"BROWSE_NODE_ID\"].unique())\n\n#create fold\nkf = model_selection.GroupKFold(n_splits = 8)\ndf['kfold'] = -1\ndf = df.sample(frac = 1).reset_index(drop = True)\ny = df.BROWSE_NODE_ID.values\nfor f, (t_, v_) in enumerate(kf.split(X = df, y = y, groups = df.TITLE.values)):\n    df.loc[v_,'kfold'] = f\n    \ndf.to_csv(\"new.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:21:42.475703Z","iopub.execute_input":"2021-08-01T15:21:42.476216Z","iopub.status.idle":"2021-08-01T15:21:52.003915Z","shell.execute_reply.started":"2021-08-01T15:21:42.476174Z","shell.execute_reply":"2021-08-01T15:21:52.00288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 64\nROBERTA_PATH = \"../input/roberta-base\"\nTOKENIZER_PATH = \"../input/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:49.473073Z","iopub.execute_input":"2021-08-01T17:01:49.473598Z","iopub.status.idle":"2021-08-01T17:01:49.883313Z","shell.execute_reply.started":"2021-08-01T17:01:49.473550Z","shell.execute_reply":"2021-08-01T17:01:49.882183Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.TITLE.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.BROWSE_NODE_ID.values, dtype=torch.long)        \n        self.MAX_LEN = 128\n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            max_length = self.MAX_LEN,\n            padding='max_length',\n            return_attention_mask=True,\n            truncation=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:21.282479Z","iopub.execute_input":"2021-08-01T17:01:21.283002Z","iopub.status.idle":"2021-08-01T17:01:21.295007Z","shell.execute_reply.started":"2021-08-01T17:01:21.282949Z","shell.execute_reply":"2021-08-01T17:01:21.293698Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# a = LitDataset(df)\n# temp = a.__getitem__(900)\n# temp","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:22.187680Z","iopub.execute_input":"2021-08-01T17:01:22.188023Z","iopub.status.idle":"2021-08-01T17:01:22.195814Z","shell.execute_reply.started":"2021-08-01T17:01:22.187993Z","shell.execute_reply":"2021-08-01T17:01:22.194649Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start = 8, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        #print(all_layer_embedding.shape)\n        #print(self.layer_weights.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        #print(weight_factor.shape)\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        #print(\"Weighted average\", weighted_average.shape)\n        return weighted_average\n    \nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                      \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config = config)  \n        self.pooler = WeightedLayerPooling(config.num_hidden_layers, layer_start = 9, layer_weights = None)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 1536),            \n            nn.ReLU(),    \n            nn.Dropout(0.1),\n            nn.Linear(1536, 3072),\n            nn.ReLU(),\n            nn.Linear(3072, 6144),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(6144, 9919),\n        )        \n\n  \n        \n\n    def forward(self, input_ids, attention_mask):\n        output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)     \n        all_hidden_states = torch.stack(output[2])\n        weighted_pooling_embeddings = self.pooler(all_hidden_states)\n        weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n        #print(pooled.shape)\n        #print(roberta_output[0].shape)\n        out = self.attention(weighted_pooling_embeddings)\n        #print(out.shape)\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:23.449465Z","iopub.execute_input":"2021-08-01T17:01:23.449817Z","iopub.status.idle":"2021-08-01T17:01:23.462674Z","shell.execute_reply.started":"2021-08-01T17:01:23.449785Z","shell.execute_reply":"2021-08-01T17:01:23.461466Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def ce_loss(\n    pred, truth, smoothing=False, trg_pad_idx=-1, eps=0.1\n):\n    '''pred = np.argmax(pred, axis = 1)\n    print(pred.shape)'''\n    \n    truth = truth.contiguous().view(-1)\n\n    one_hot = torch.zeros_like(pred).scatter(1, truth.view(-1, 1), 1)\n\n    if smoothing:\n        n_class = pred.size(1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n\n    loss = -one_hot * F.log_softmax(pred, dim=1)\n\n    if trg_pad_idx >= 0:\n        loss = loss.sum(dim=1)\n        non_pad_mask = truth.ne(trg_pad_idx)\n        loss = loss.masked_select(non_pad_mask)\n\n    return loss.sum()\n\n\ndef loss_fn(output, target):\n    \n    \n    #bs = output.size(0)\n\n    loss = ce_loss(\n        output,\n        target,\n        smoothing=False,\n        eps=0.1,\n    )\n\n\n    return loss ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:25.008199Z","iopub.execute_input":"2021-08-01T17:01:25.008625Z","iopub.status.idle":"2021-08-01T17:01:25.017921Z","shell.execute_reply.started":"2021-08-01T17:01:25.008588Z","shell.execute_reply":"2021-08-01T17:01:25.016286Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        '''config.update({\"output_hidden_states\": False,\n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})     '''                  \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config = config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.ReLU(),                       \n            nn.Linear(512, 9919),\n        )        \n\n  \n        \n\n    def forward(self, input_ids, attention_mask):\n        output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)     \n        pooled = output[1]\n        #print(pooled.shape)\n        #print(roberta_output[0].shape)\n        out = self.attention(pooled)\n        #print(out.shape)\n        return out\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:21:52.48821Z","iopub.execute_input":"2021-08-01T15:21:52.488531Z","iopub.status.idle":"2021-08-01T15:21:52.501713Z","shell.execute_reply.started":"2021-08-01T15:21:52.488502Z","shell.execute_reply":"2021-08-01T15:21:52.500466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    average_loss = 0\n    for i, (ids, attention, target) in enumerate(tk0):\n\n        ids =ids\n        attention = attention\n        target = target\n\n        ids = ids.to(device, dtype=torch.long)\n        attention = attention.to(device, dtype=torch.long)\n        target = target.to(device, dtype=torch.long)\n\n        #print(ids.shape, token_type_ids.shape, mask.shape)\n        model.zero_grad()\n        outputs  = model(\n            input_ids=ids,\n            attention_mask=attention,\n        )\n        #print(outputs,  target)\n        loss = loss_fn(outputs ,target)\n        average_loss += loss.item()\n        #print(loss)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    print(f\"average_loss is {average_loss/len(data_loader)}\")\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:31.376273Z","iopub.execute_input":"2021-08-01T17:01:31.376683Z","iopub.status.idle":"2021-08-01T17:01:31.384953Z","shell.execute_reply.started":"2021-08-01T17:01:31.376647Z","shell.execute_reply":"2021-08-01T17:01:31.383698Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def eval_fn(data_loader, model, device):\n    model.eval()\n\n    final_targets = []\n    final_outputs = []\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    with torch.no_grad():\n        for i, (ids, attention, target) in enumerate(tk0):\n\n            ids = ids\n            attention = attention\n            target = target\n\n            ids = ids.to(device, dtype=torch.long)\n            attention = attention.to(device, dtype=torch.long)\n            target = target.to(device, dtype=torch.long)\n\n            #print(ids.shape, token_type_ids.shape, mask.shape)\n            outputs  = model(\n                input_ids=ids,\n                attention_mask=attention,\n            )\n            \n            target = target.detach().cpu().numpy().tolist()\n            output = outputs.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(target)\n            final_outputs.extend(outputs)\n            \n    return final_outputs, final_targets\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:32.133441Z","iopub.execute_input":"2021-08-01T17:01:32.133854Z","iopub.status.idle":"2021-08-01T17:01:32.141170Z","shell.execute_reply.started":"2021-08-01T17:01:32.133816Z","shell.execute_reply":"2021-08-01T17:01:32.140323Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:01:34.237160Z","iopub.execute_input":"2021-08-01T17:01:34.237607Z","iopub.status.idle":"2021-08-01T17:01:34.248524Z","shell.execute_reply.started":"2021-08-01T17:01:34.237568Z","shell.execute_reply":"2021-08-01T17:01:34.247315Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def run(fold):\n    dfx = pd.read_csv(\"./new.csv\")\n    model_path = f\"model_{fold + 1}.pth\"\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    best_val_rmse = None\n    \n    train_dataset = LitDataset(df_train)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size= 16,\n        num_workers=4,\n        shuffle = True\n    )\n\n    valid_dataset = LitDataset(df_valid)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size= 1,\n        num_workers = 2\n    )\n\n    device = torch.device(\"cuda\")\n    model = LitModel()\n    model.to(device)\n\n    num_train_steps = int(len(df_train) / 16 * 3)\n \n    optimizer = create_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps= 50, \n        num_training_steps=num_train_steps\n    )\n\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # I'm training only for 3 epochs even though I specified 5!!!\n    for epoch in range(3):\n        print(\"===================start training===================\")\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        print(\"===================validation=======================\")\n        pred, target = eval_fn(valid_data_loader, model, device)\n        print(len(pred), len(target))\n        \n        if not best_val_rmse:\n          torch.save(model.state_dict(), model_path)\n        \n        correct_pred = 0\n        for pre, tar in zip(pred, target):\n            _, prediction_indices = torch.max(pre, dim = 0)\n            correct_pred += (prediction_indices == tar).float()\n        acc = correct_pred.sum() / len(target)\n        acc = torch.round(acc * 100)\n        \n        if not best_val_rmse or acc < best_val_rmse:                    \n            best_val_rmse = acc\n            best_epoch = epoch\n            torch.save(model.state_dict(), model_path)\n            print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n        else:       \n            print(f\"Still best_val: {best_val_rmse:0.4}\",\n                    f\"(from epoch {best_epoch})\")\n            \n        print(f\"epochs {epoch} , validation accuracy is {acc}\")\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:03:34.278294Z","iopub.execute_input":"2021-08-01T16:03:34.278702Z","iopub.status.idle":"2021-08-01T16:03:34.292318Z","shell.execute_reply.started":"2021-08-01T16:03:34.278668Z","shell.execute_reply":"2021-08-01T16:03:34.291304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(fold = 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:03:35.30736Z","iopub.execute_input":"2021-08-01T16:03:35.307727Z","iopub.status.idle":"2021-08-01T16:40:32.967849Z","shell.execute_reply.started":"2021-08-01T16:03:35.307694Z","shell.execute_reply":"2021-08-01T16:40:32.960934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(fold=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:58:22.287058Z","iopub.status.idle":"2021-08-01T15:58:22.287811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(fold=2)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:58:22.289234Z","iopub.status.idle":"2021-08-01T15:58:22.289956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:57:16.637085Z","iopub.execute_input":"2021-08-01T16:57:16.637470Z","iopub.status.idle":"2021-08-01T16:57:16.642286Z","shell.execute_reply.started":"2021-08-01T16:57:16.637434Z","shell.execute_reply":"2021-08-01T16:57:16.640948Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:57:36.023340Z","iopub.execute_input":"2021-08-01T16:57:36.023802Z","iopub.status.idle":"2021-08-01T16:57:36.327152Z","shell.execute_reply.started":"2021-08-01T16:57:36.023762Z","shell.execute_reply":"2021-08-01T16:57:36.324928Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/amazon-ml-challenge-2021-hackerearth/test.csv\", error_bad_lines = False, escapechar = \"\\\\\" , quoting = csv.QUOTE_NONE)\ntest_df['TITLE'] = test_df['TITLE'].fillna('a')\n# test_dataset = LitDataset(test_df, inference_only=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:00:53.599976Z","iopub.execute_input":"2021-08-01T17:00:53.600400Z","iopub.status.idle":"2021-08-01T17:00:57.203015Z","shell.execute_reply.started":"2021-08-01T17:00:53.600338Z","shell.execute_reply":"2021-08-01T17:00:57.202123Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"del test_dataset","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:58:34.474328Z","iopub.execute_input":"2021-08-01T16:58:34.474745Z","iopub.status.idle":"2021-08-01T16:58:36.480505Z","shell.execute_reply.started":"2021-08-01T16:58:34.474710Z","shell.execute_reply":"2021-08-01T16:58:36.477319Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:13:04.869263Z","iopub.execute_input":"2021-08-01T17:13:04.869683Z","iopub.status.idle":"2021-08-01T17:13:04.908601Z","shell.execute_reply.started":"2021-08-01T17:13:04.869647Z","shell.execute_reply":"2021-08-01T17:13:04.907577Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"        PRODUCT_ID                                              TITLE  \\\n0                1  Command 3M Small Kitchen Hooks, White, Decorat...   \n1                2  O'Neal Jump Hardware JAG Unisex-Adult Glove (B...   \n2                3  NFL Detroit Lions Portable Party Fridge, 15.8 ...   \n3                4  Panasonic Single Line KX-TS880MX Corded Phone ...   \n4                5  Zero Baby Girl's 100% Cotton Innerwear Bloomer...   \n...            ...                                                ...   \n110770      110771  AAHNA E MALL OneBlade Hybrid Trimmer Shaver An...   \n110771      110772  Grin Health N99 Anti Pollution Reusable Washab...   \n110772      110773  Asian Army Pink Ultra reusable respirator clot...   \n110773      110774  IM Safe 3 Ply Non-Woven Disposable Surgical Fa...   \n110774      110775  Artifie 100% Cotton Unisex Reusable Breathable...   \n\n                                              DESCRIPTION  \\\n0                                         Sale Unit: PACK   \n1       Synthetic leather palm with double-layer thumb...   \n2       Boelter Brands lets you celebrate your favorit...   \n3       Features: 50 Station Phonebook Corded Phone Al...   \n4       Zero Baby Girl Panties Set. 100% Cotton, Breat...   \n...                                                   ...   \n110770  <p>1-All In One Hyper Advanced Smart Rechargea...   \n110771  <p>SIZE GUIDE : M - (35- 65 Kg), L - (49- 72 K...   \n110772  Asian HyperProtect A95 masks have been enginee...   \n110773  This 3 Ply Disposable face mask is manufacture...   \n110774  Material : Cotton Printed, cotton thread, Soft...   \n\n                                            BULLET_POINTS           BRAND  \n0       [INCLUDES - 9 hooks and 12 small indoor strips...         Command  \n1       [Silicone printing for a better grip. Long las...          O'Neal  \n2       [Runs on 12 Volt DC Power or 110 Volt AC Power...  Boelter Brands  \n3       Panasonic Landline Phones doesn't come with a ...       Panasonic  \n4       [Zero Baby Girl Panties, Pack of 6, 100% Cotto...            Zero  \n...                                                   ...             ...  \n110770  [Unique One Blade can style, trim and shave, w...         Generic  \n110771  [PROTECTION: Filtration rate up to ≥99 percent...     Grin Health  \n110772  [Reusable and environment friendly: These mask...           ASIAN  \n110773  [3 Ply Mask: Genuine 3 Ply Mask. 25 GSM Spun B...     Intermarket  \n110774  [Eco Friendly, Reusable Cotton Masks for Face ...         Artifie  \n\n[110775 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PRODUCT_ID</th>\n      <th>TITLE</th>\n      <th>DESCRIPTION</th>\n      <th>BULLET_POINTS</th>\n      <th>BRAND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Command 3M Small Kitchen Hooks, White, Decorat...</td>\n      <td>Sale Unit: PACK</td>\n      <td>[INCLUDES - 9 hooks and 12 small indoor strips...</td>\n      <td>Command</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>O'Neal Jump Hardware JAG Unisex-Adult Glove (B...</td>\n      <td>Synthetic leather palm with double-layer thumb...</td>\n      <td>[Silicone printing for a better grip. Long las...</td>\n      <td>O'Neal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NFL Detroit Lions Portable Party Fridge, 15.8 ...</td>\n      <td>Boelter Brands lets you celebrate your favorit...</td>\n      <td>[Runs on 12 Volt DC Power or 110 Volt AC Power...</td>\n      <td>Boelter Brands</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Panasonic Single Line KX-TS880MX Corded Phone ...</td>\n      <td>Features: 50 Station Phonebook Corded Phone Al...</td>\n      <td>Panasonic Landline Phones doesn't come with a ...</td>\n      <td>Panasonic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Zero Baby Girl's 100% Cotton Innerwear Bloomer...</td>\n      <td>Zero Baby Girl Panties Set. 100% Cotton, Breat...</td>\n      <td>[Zero Baby Girl Panties, Pack of 6, 100% Cotto...</td>\n      <td>Zero</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110770</th>\n      <td>110771</td>\n      <td>AAHNA E MALL OneBlade Hybrid Trimmer Shaver An...</td>\n      <td>&lt;p&gt;1-All In One Hyper Advanced Smart Rechargea...</td>\n      <td>[Unique One Blade can style, trim and shave, w...</td>\n      <td>Generic</td>\n    </tr>\n    <tr>\n      <th>110771</th>\n      <td>110772</td>\n      <td>Grin Health N99 Anti Pollution Reusable Washab...</td>\n      <td>&lt;p&gt;SIZE GUIDE : M - (35- 65 Kg), L - (49- 72 K...</td>\n      <td>[PROTECTION: Filtration rate up to ≥99 percent...</td>\n      <td>Grin Health</td>\n    </tr>\n    <tr>\n      <th>110772</th>\n      <td>110773</td>\n      <td>Asian Army Pink Ultra reusable respirator clot...</td>\n      <td>Asian HyperProtect A95 masks have been enginee...</td>\n      <td>[Reusable and environment friendly: These mask...</td>\n      <td>ASIAN</td>\n    </tr>\n    <tr>\n      <th>110773</th>\n      <td>110774</td>\n      <td>IM Safe 3 Ply Non-Woven Disposable Surgical Fa...</td>\n      <td>This 3 Ply Disposable face mask is manufacture...</td>\n      <td>[3 Ply Mask: Genuine 3 Ply Mask. 25 GSM Spun B...</td>\n      <td>Intermarket</td>\n    </tr>\n    <tr>\n      <th>110774</th>\n      <td>110775</td>\n      <td>Artifie 100% Cotton Unisex Reusable Breathable...</td>\n      <td>Material : Cotton Printed, cotton thread, Soft...</td>\n      <td>[Eco Friendly, Reusable Cotton Masks for Face ...</td>\n      <td>Artifie</td>\n    </tr>\n  </tbody>\n</table>\n<p>110775 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class test_infer(Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.title  = df.TITLE.tolist()\n        self.id = df.PRODUCT_ID.values\n        self.encoded = tokenizer.batch_encode_plus(\n            self.title,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self,index):\n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n   \n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n\n        product_id = self.id[index]\n\n        return (\n            input_ids,\n            attention_mask,\n            product_id\n        )\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:44:57.831100Z","iopub.execute_input":"2021-08-01T17:44:57.831507Z","iopub.status.idle":"2021-08-01T17:44:57.840514Z","shell.execute_reply.started":"2021-08-01T17:44:57.831461Z","shell.execute_reply":"2021-08-01T17:44:57.839412Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def predict_fn(data_loader, model, device):\n    model.eval()\n\n    final_outputs = []\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    with torch.no_grad():\n        for i, (ids, attention, product_id) in enumerate(tk0):\n\n            ids = ids\n            attention = attention\n            product_id = product_id\n            \n   \n\n            ids = ids.to(device, dtype=torch.long)\n            attention = attention.to(device, dtype=torch.long)\n            #product_id = product_id.to(device, dtype=torch.long)\n\n            #print(ids.shape, token_type_ids.shape, mask.shape)\n            outputs  = model(\n                input_ids=ids,\n                attention_mask=attention,\n            )\n            \n            product_id = product_id.numpy().tolist()\n            \n            output = outputs.detach().cpu().numpy().tolist()\n            final_outputs.extend(outputs)\n            \n    return product_id, final_outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:45:45.684486Z","iopub.execute_input":"2021-08-01T17:45:45.684892Z","iopub.status.idle":"2021-08-01T17:45:45.699202Z","shell.execute_reply.started":"2021-08-01T17:45:45.684854Z","shell.execute_reply":"2021-08-01T17:45:45.697629Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model = LitModel()\nmodel.load_state_dict(torch.load('model_1.pth'))\nmodel.to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:43:19.270262Z","iopub.execute_input":"2021-08-01T17:43:19.270713Z","iopub.status.idle":"2021-08-01T17:43:22.351697Z","shell.execute_reply.started":"2021-08-01T17:43:19.270678Z","shell.execute_reply":"2021-08-01T17:43:22.350840Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"LitModel(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (pooler): WeightedLayerPooling()\n  (attention): Sequential(\n    (0): Linear(in_features=768, out_features=1536, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=1536, out_features=3072, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=3072, out_features=6144, bias=True)\n    (6): ReLU()\n    (7): Dropout(p=0.2, inplace=False)\n    (8): Linear(in_features=6144, out_features=9919, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"temp = test_infer(test_df)\ntemp.__getitem__(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:40:30.581726Z","iopub.execute_input":"2021-08-01T17:40:30.582129Z","iopub.status.idle":"2021-08-01T17:40:41.624966Z","shell.execute_reply.started":"2021-08-01T17:40:30.582096Z","shell.execute_reply":"2021-08-01T17:40:41.623702Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"tensor([    0, 46785,   155,   448,  7090, 11580, 14943,    29,     6,   735,\n            6,  1502, 28590, 33571,  3130,     6, 18609,   374,     6, 18609,\n         4995,     6,   361, 14943,    29,     6,   316, 21836,  3275,     6,\n        19268,    12, 31331,   111, 12641,  4111,    12, 12015,     2,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1])\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(tensor([    0, 46785,   155,   448,  7090, 11580, 14943,    29,     6,   735,\n             6,  1502, 28590, 33571,  3130,     6, 18609,   374,     6, 18609,\n          4995,     6,   361, 14943,    29,     6,   316, 21836,  3275,     6,\n         19268,    12, 31331,   111, 12641,  4111,    12, 12015,     2,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1]),\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 1)"},"metadata":{}}]},{"cell_type":"code","source":"dataLoader = torch.utils.data.DataLoader(\n        test_infer(test_df),\n        batch_size= 16,\n        num_workers=4,\n    )\n\nproduct_id, final_outputs = predict_fn(dataLoader,model,DEVICE)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:45:51.513469Z","iopub.execute_input":"2021-08-01T17:45:51.513860Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 55%|█████▍    | 3787/6924 [03:40<03:08, 16.63it/s]","output_type":"stream"}]},{"cell_type":"code","source":"\n\nout = []\n\nmap_rev = {v : k for k, v in map_values.items()}\n\nfor pre in final_outputs:\n    pre_ind = torch.max(pre,dim=0)\n    out.append(map_rev[pre_ind])\n\ndf_sub = pd.DataFrame(list(zip(product_id, out)),\n               columns =['PRODUCT_ID', 'BROWSE_NODE_ID'])\ndf_sub.to_csv(\"sub1.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}